{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da03ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71447a",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa5d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list = ['tweet_id', 'user_id', 'bullying_traces', 'type', 'form', 'teasing', 'author_role', 'emotion']\n",
    "csv = pd.read_csv(\"data.csv\", names=header_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68263476",
   "metadata": {},
   "source": [
    "### Examination of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc84001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7321, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9715d2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>bullying_traces</th>\n",
       "      <th>type</th>\n",
       "      <th>form</th>\n",
       "      <th>teasing</th>\n",
       "      <th>author_role</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105730486382497793</td>\n",
       "      <td>322329899</td>\n",
       "      <td>y</td>\n",
       "      <td>self-disclosure</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>victim</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107688644067856384</td>\n",
       "      <td>185389094</td>\n",
       "      <td>y</td>\n",
       "      <td>self-disclosure</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>defender</td>\n",
       "      <td>empathy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108654820042354688</td>\n",
       "      <td>361869056</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102206417217392640</td>\n",
       "      <td>226320672</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102779484725448704</td>\n",
       "      <td>297557032</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    user_id bullying_traces             type   form  \\\n",
       "0  105730486382497793  322329899               y  self-disclosure  other   \n",
       "1  107688644067856384  185389094               y  self-disclosure  other   \n",
       "2  108654820042354688  361869056               n              NaN    NaN   \n",
       "3  102206417217392640  226320672               n              NaN    NaN   \n",
       "4  102779484725448704  297557032               n              NaN    NaN   \n",
       "\n",
       "  teasing author_role  emotion  \n",
       "0       n      victim     none  \n",
       "1       n    defender  empathy  \n",
       "2     NaN         NaN      NaN  \n",
       "3     NaN         NaN      NaN  \n",
       "4     NaN         NaN      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prikaz skupa za treniranje\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6de56d",
   "metadata": {},
   "source": [
    "### Feature description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e1b991",
   "metadata": {},
   "source": [
    "* **tweet_id** - Tweet identification number\n",
    "* **user_id** - User identification number\n",
    "* **bullying_traces** - Boolean value that says if tweet id a bullying trace\n",
    "* **type** - Type of bullying(accusation, cyberbullying, denial, report, self-disclosure, NA)\n",
    "* **form** - Form of bullying(cyberbullying, physical, property damage, relational, verbal, other, NA)\n",
    "* **teasing** - Boolean that says if teasing is involved\n",
    "* **author_role** - Author role in a tweet(accuser, assistant, bully, defender, reinforcer, reporter, victim, other, NA)\n",
    "* **emotion** - Emotion expressed in a tweet(anger, embarrassment, empathy, fear, none, other, pride, relief, sadness, NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5afbe2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id              0\n",
       "user_id               0\n",
       "bullying_traces       0\n",
       "type               5219\n",
       "form               5219\n",
       "teasing            5219\n",
       "author_role        5219\n",
       "emotion            5219\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(csv.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f30403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.321000e+03</td>\n",
       "      <td>7.321000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.033140e+17</td>\n",
       "      <td>1.709711e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.121057e+15</td>\n",
       "      <td>1.062885e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.970695e+16</td>\n",
       "      <td>7.794510e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000126e+17</td>\n",
       "      <td>7.045550e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.029122e+17</td>\n",
       "      <td>1.644135e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.061250e+17</td>\n",
       "      <td>2.646067e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.091281e+17</td>\n",
       "      <td>3.652028e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id       user_id\n",
       "count  7.321000e+03  7.321000e+03\n",
       "mean   1.033140e+17  1.709711e+08\n",
       "std    3.121057e+15  1.062885e+08\n",
       "min    9.970695e+16  7.794510e+05\n",
       "25%    1.000126e+17  7.045550e+07\n",
       "50%    1.029122e+17  1.644135e+08\n",
       "75%    1.061250e+17  2.646067e+08\n",
       "max    1.091281e+17  3.652028e+08"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8fce42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id           7298\n",
       "user_id            7043\n",
       "bullying_traces       2\n",
       "type                  5\n",
       "form                  6\n",
       "teasing               2\n",
       "author_role           8\n",
       "emotion               9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be64941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7321 entries, 0 to 7320\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   tweet_id         7321 non-null   int64 \n",
      " 1   user_id          7321 non-null   int64 \n",
      " 2   bullying_traces  7321 non-null   object\n",
      " 3   type             2102 non-null   object\n",
      " 4   form             2102 non-null   object\n",
      " 5   teasing          2102 non-null   object\n",
      " 6   author_role      2102 non-null   object\n",
      " 7   emotion          2102 non-null   object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 457.7+ KB\n"
     ]
    }
   ],
   "source": [
    "csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f22c78e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>bullying_traces</th>\n",
       "      <th>type</th>\n",
       "      <th>form</th>\n",
       "      <th>teasing</th>\n",
       "      <th>author_role</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105730486382497793</td>\n",
       "      <td>322329899</td>\n",
       "      <td>y</td>\n",
       "      <td>self-disclosure</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>victim</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107688644067856384</td>\n",
       "      <td>185389094</td>\n",
       "      <td>y</td>\n",
       "      <td>self-disclosure</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>defender</td>\n",
       "      <td>empathy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108654820042354688</td>\n",
       "      <td>361869056</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102206417217392640</td>\n",
       "      <td>226320672</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102779484725448704</td>\n",
       "      <td>297557032</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7316</th>\n",
       "      <td>99912247060611074</td>\n",
       "      <td>162589166</td>\n",
       "      <td>y</td>\n",
       "      <td>self-disclosure</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>defender</td>\n",
       "      <td>empathy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7317</th>\n",
       "      <td>99878062220517376</td>\n",
       "      <td>221054960</td>\n",
       "      <td>y</td>\n",
       "      <td>accusation</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>accuser</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7318</th>\n",
       "      <td>100049839949221889</td>\n",
       "      <td>110032215</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>100006640492875776</td>\n",
       "      <td>171842492</td>\n",
       "      <td>y</td>\n",
       "      <td>denial</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>bully</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7320</th>\n",
       "      <td>99712633174888448</td>\n",
       "      <td>225505498</td>\n",
       "      <td>y</td>\n",
       "      <td>report</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>reporter</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7321 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id    user_id bullying_traces             type   form  \\\n",
       "0     105730486382497793  322329899               y  self-disclosure  other   \n",
       "1     107688644067856384  185389094               y  self-disclosure  other   \n",
       "2     108654820042354688  361869056               n              NaN    NaN   \n",
       "3     102206417217392640  226320672               n              NaN    NaN   \n",
       "4     102779484725448704  297557032               n              NaN    NaN   \n",
       "...                  ...        ...             ...              ...    ...   \n",
       "7316   99912247060611074  162589166               y  self-disclosure  other   \n",
       "7317   99878062220517376  221054960               y       accusation  other   \n",
       "7318  100049839949221889  110032215               n              NaN    NaN   \n",
       "7319  100006640492875776  171842492               y           denial  other   \n",
       "7320   99712633174888448  225505498               y           report  other   \n",
       "\n",
       "     teasing author_role  emotion  \n",
       "0          n      victim     none  \n",
       "1          n    defender  empathy  \n",
       "2        NaN         NaN      NaN  \n",
       "3        NaN         NaN      NaN  \n",
       "4        NaN         NaN      NaN  \n",
       "...      ...         ...      ...  \n",
       "7316       n    defender  empathy  \n",
       "7317       n     accuser     none  \n",
       "7318     NaN         NaN      NaN  \n",
       "7319       n       bully     none  \n",
       "7320       n    reporter     none  \n",
       "\n",
       "[7321 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1e286",
   "metadata": {},
   "source": [
    "### Tweet overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fac0cd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Underwalt @MsShandraRae @kateplusmy8 mr. walt, stop being a bully.\n",
      "【自動post】BULLY（いじめ）とSUICIDE（自殺）で、BULLYCIDE（いじめ自殺）。もっとも、自殺するのは憎い相手を殺した後。死なばもろとも…ってところかしら\n",
      "Paula, Kiersten and LJ's song about bullying. Great job! (Uploading more videos now.) http://fb.me/ASIm1gw1\n",
      "@katie_ogden @boybandslut @StephWall27 @BAMitsSHAZ @xstephhh_ I was not bullying her! She's trying to steal Jean Martyn from meee! :'( xx\n",
      "cara3: bully iam gara2 uda di ucpin tpi gk bls\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "f = open('tweet.json')\n",
    "tweets = json.load(f)\n",
    "\n",
    "counter = 0\n",
    "for tweet in tweets:\n",
    "    counter += 1\n",
    "    print(tweet['text'])\n",
    "    \n",
    "    if counter == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf8b90b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['created_at', 'id', 'id_str', 'text', 'truncated', 'entities', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'lang'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab284c",
   "metadata": {},
   "source": [
    "### Maybe important features: \n",
    "* **text** \n",
    "* **symbols** \n",
    "* **in_reply_to_status_id**\n",
    "* **in_reply_to_user_id**\n",
    "* **description**\n",
    "* **retweet_count**\n",
    "* **retweeted**\n",
    "* **favorite_count**\n",
    "* **favorited**\n",
    "* **lang**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db4b5a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:3992\n",
      "Number of english tweets:2522\n"
     ]
    }
   ],
   "source": [
    "eng_tweets = []\n",
    "eng_tweet_ids = []\n",
    "for tweet in tweets:\n",
    "    if tweet['lang'] == 'en':\n",
    "        eng_tweets.append(tweet)\n",
    "        eng_tweet_ids.append(tweet['id'])\n",
    "        \n",
    "        \n",
    "print(f'Number of tweets:{len(tweets)}')        \n",
    "print(f'Number of english tweets:{len(eng_tweets)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55b8f1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>bullying_traces</th>\n",
       "      <th>type</th>\n",
       "      <th>form</th>\n",
       "      <th>teasing</th>\n",
       "      <th>author_role</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107688644067856384</td>\n",
       "      <td>185389094</td>\n",
       "      <td>y</td>\n",
       "      <td>self-disclosure</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>defender</td>\n",
       "      <td>empathy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102779484725448704</td>\n",
       "      <td>297557032</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>109034091743154176</td>\n",
       "      <td>177913822</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>102533497637437441</td>\n",
       "      <td>70412906</td>\n",
       "      <td>y</td>\n",
       "      <td>report</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>reporter</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>103679008855691264</td>\n",
       "      <td>11363462</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7306</th>\n",
       "      <td>99823454777393152</td>\n",
       "      <td>48203749</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7307</th>\n",
       "      <td>100063708629319680</td>\n",
       "      <td>305258003</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314</th>\n",
       "      <td>99886032048226304</td>\n",
       "      <td>21235814</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7319</th>\n",
       "      <td>100006640492875776</td>\n",
       "      <td>171842492</td>\n",
       "      <td>y</td>\n",
       "      <td>denial</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>bully</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7320</th>\n",
       "      <td>99712633174888448</td>\n",
       "      <td>225505498</td>\n",
       "      <td>y</td>\n",
       "      <td>report</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "      <td>reporter</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2522 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id    user_id bullying_traces             type   form  \\\n",
       "1     107688644067856384  185389094               y  self-disclosure  other   \n",
       "4     102779484725448704  297557032               n              NaN    NaN   \n",
       "9     109034091743154176  177913822               n              NaN    NaN   \n",
       "10    102533497637437441   70412906               y           report  other   \n",
       "14    103679008855691264   11363462               n              NaN    NaN   \n",
       "...                  ...        ...             ...              ...    ...   \n",
       "7306   99823454777393152   48203749               n              NaN    NaN   \n",
       "7307  100063708629319680  305258003               n              NaN    NaN   \n",
       "7314   99886032048226304   21235814               n              NaN    NaN   \n",
       "7319  100006640492875776  171842492               y           denial  other   \n",
       "7320   99712633174888448  225505498               y           report  other   \n",
       "\n",
       "     teasing author_role  emotion  \n",
       "1          n    defender  empathy  \n",
       "4        NaN         NaN      NaN  \n",
       "9        NaN         NaN      NaN  \n",
       "10         n    reporter     none  \n",
       "14       NaN         NaN      NaN  \n",
       "...      ...         ...      ...  \n",
       "7306     NaN         NaN      NaN  \n",
       "7307     NaN         NaN      NaN  \n",
       "7314     NaN         NaN      NaN  \n",
       "7319       n       bully     none  \n",
       "7320       n    reporter     none  \n",
       "\n",
       "[2522 rows x 8 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we filter out non english tweets\n",
    "eng_tweets_csv = csv.loc[csv[\"tweet_id\"].isin(eng_tweet_ids)]\n",
    "eng_tweets_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c42d0a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n    1495\n",
       "y    1027\n",
       "Name: bullying_traces, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tweets_csv['bullying_traces'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d14d3b",
   "metadata": {},
   "source": [
    "#### Tweet normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0e2c41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL ... via @USER\n"
     ]
    }
   ],
   "source": [
    "from emoji import demojize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"@USER\"\n",
    "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "        return \"HTTPURL\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        if token == \"’\":\n",
    "            return \"'\"\n",
    "        elif token == \"…\":\n",
    "            return \"...\"\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "\n",
    "def normalizeTweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"cannot \", \"can not \")\n",
    "        .replace(\"n't \", \" n't \")\n",
    "        .replace(\"n 't \", \" n't \")\n",
    "        .replace(\"ca n't\", \"can't\")\n",
    "        .replace(\"ai n't\", \"ain't\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"'m \", \" 'm \")\n",
    "        .replace(\"'re \", \" 're \")\n",
    "        .replace(\"'s \", \" 's \")\n",
    "        .replace(\"'ll \", \" 'll \")\n",
    "        .replace(\"'d \", \" 'd \")\n",
    "        .replace(\"'ve \", \" 've \")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\" p . m .\", \"  p.m.\")\n",
    "        .replace(\" p . m \", \" p.m \")\n",
    "        .replace(\" a . m .\", \" a.m.\")\n",
    "        .replace(\" a . m \", \" a.m \")\n",
    "    )\n",
    "\n",
    "    return \" \".join(normTweet.split())\n",
    "\n",
    "\n",
    "\n",
    "print(\n",
    "    normalizeTweet(\n",
    "        \"SC has first two presumptive cases of coronavirus, DHEC confirms https://postandcourier.com/health/covid19/sc-has-first-two-presumptive-cases-of-coronavirus-dhec-confirms/article_bddfe4ae-5fd3-11ea-9ce4-5f495366cee6.html?utm_medium=social&utm_source=twitter&utm_campaign=user-share… via @postandcourier\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33f00c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DHEC confirms HTTPURL ... via @USER :crying_face:\n"
     ]
    }
   ],
   "source": [
    "print(normalizeTweet(\"DHEC confirms https://postandcourier.com/health/covid19/sc-has-first-two-presumptive-cases-of-coronavirus-dhec-confirms/article_bddfe4ae-5fd3-11ea-9ce4-5f495366cee6.html?utm_medium=social&utm_source=twitter&utm_campaign=user-share… via @postandcourier 😢\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68a3f228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Underwalt @MsShandraRae @kateplusmy8 mr. walt, stop being a bully.\n",
      "@USER @USER @USER mr . walt , stop being a bully .\n"
     ]
    }
   ],
   "source": [
    "tweet_number = 0\n",
    "\n",
    "print(eng_tweets[tweet_number]['text'])\n",
    "print(normalizeTweet(eng_tweets[tweet_number]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4dc2d",
   "metadata": {},
   "source": [
    "#### BERT feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c545a630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-large\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")\n",
    "\n",
    "# INPUT TWEET IS ALREADY NORMALIZED!\n",
    "line = \"DHEC confirms HTTPURL via @USER :crying_face:\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = bertweet(input_ids)  # Models outputs are now tuples\n",
    "    \n",
    "## With TensorFlow 2.0+:\n",
    "# from transformers import TFAutoModel\n",
    "# bertweet = TFAutoModel.from_pretrained(\"vinai/bertweet-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b4abe55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0862,  0.0521, -0.1026,  ...,  0.0147,  0.1891, -0.0736],\n",
       "         [ 0.1007, -0.0159, -0.6580,  ...,  0.1054,  0.2050,  0.0699],\n",
       "         [ 0.2216,  0.0395, -0.5482,  ..., -0.1893,  0.3768,  0.2154],\n",
       "         ...,\n",
       "         [ 0.0523,  0.2308, -0.8912,  ...,  0.3122, -0.0351, -0.0617],\n",
       "         [-0.0252, -0.1803, -0.0527,  ...,  0.3899, -0.1398, -0.2169],\n",
       "         [ 0.0499,  0.0034, -0.1559,  ..., -0.0495,  0.1139, -0.0543]]]), pooler_output=tensor([[-0.0585,  0.0388,  0.3094,  ..., -0.0328, -0.1053,  0.3659]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d69bbdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector size for each word: 1024\n"
     ]
    }
   ],
   "source": [
    "print('vector size for each word:',len(features[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c6e125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5a20a",
   "metadata": {},
   "source": [
    "#### Preparing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "809c1794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USER @USER @USER mr . walt , stop being a bully .\n",
      "Paula , Kiersten and LJ 's song about bullying . Great job ! ( Uploading more videos now . ) HTTPURL\n",
      "@USER @USER @USER @USER @USER I was not bullying her ! She 's trying to steal Jean Martyn from meee ! :'( xx\n",
      "This is actually bullying , I want Wenger to watch this and fucking hold every piece of criticism that comes ! #RAGE\n",
      "If I 'm not happy with my classes this week , I 'm gonna take the role as a bully and be an asshole . Watch out lower-classmen . It will be ugly .\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the tweets\n",
    "for i in range(len(eng_tweets)):\n",
    "    eng_tweets[i]['text'] = normalizeTweet(eng_tweets[i]['text'])\n",
    "\n",
    "counter = 0\n",
    "for tweet in eng_tweets:\n",
    "    counter += 1\n",
    "    print(tweet['text'])\n",
    "    \n",
    "    if counter == 5:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "73b598b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tweets_csv.loc[eng_tweets_csv[\"tweet_id\"]==eng_tweets[1]['id']]['bullying_traces'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba361733",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([tokenizer\u001b[38;5;241m.\u001b[39mencode(tweet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 9\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mbertweet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     x\u001b[38;5;241m.\u001b[39mappend(features)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:851\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    842\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    844\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    845\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    846\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    849\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    850\u001b[0m )\n\u001b[1;32m--> 851\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    864\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:454\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    451\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    452\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 454\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\transformers\\modeling_utils.py:2928\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   2925\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m   2926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m-> 2928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:466\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 466\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:365\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 365\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\aipt\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for tweet in eng_tweets:\n",
    "    y.append(eng_tweets_csv.loc[eng_tweets_csv[\"tweet_id\"]==tweet['id']]['bullying_traces'].values[0])\n",
    "    input_ids = torch.tensor([tokenizer.encode(tweet['text'])])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = bertweet(input_ids)\n",
    "        x.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bc1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1eda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7171f604",
   "metadata": {},
   "source": [
    "#### Custom torch model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
